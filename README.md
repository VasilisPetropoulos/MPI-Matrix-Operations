# Parallel Matrix Operations using MPI

Αυτό το repository περιέχει μια υλοποίηση παράλληλων αλγορίθμων για βασικές πράξεις γραμμικής άλγεβρας, χρησιμοποιώντας το πρότυπο **MPI (Message Passing Interface)** στη γλώσσα C.

Ο στόχος του προγράμματος είναι η αποδοτική εκμετάλλευση κατανεμημένων συστημάτων για την επεξεργασία πινάκων και διανυσμάτων, χρησιμοποιώντας τεχνικές διαμοιρασμού δεδομένων (data decomposition) και προηγμένα μοτίβα επικοινωνίας.

## 🚀 Δυνατότητες (Features)

Το πρόγραμμα υποστηρίζει τις ακόλουθες πράξεις για πίνακες $N \times N$ και διανύσματα $N \times 1$:

1.  **Πρόσθεση Πινάκων ($C + D$):** Παράλληλος υπολογισμός με διαμοιρασμό δεδομένων.
2.  **Πολλαπλασιασμός Πίνακα με Διάνυσμα ($C \times B$):** Χρήση Broadcast για το διάνυσμα και Scatter για τον πίνακα.
3.  **Εσωτερικό Γινόμενο Διανυσμάτων ($A \cdot B$):** Κατανεμημένος υπολογισμός με τελική αναγωγή (Reduction).
4.  **Πολλαπλασιασμός Πινάκων ($C \times D$) - Ring Algorithm:** Υλοποίηση του αλγορίθμου δακτυλίου (Ring Topology) με ασύγχρονη επικοινωνία.

## 🛠️ Αρχιτεκτονική & Λογική Υλοποίησης

Η εφαρμογή ακολουθεί το μοντέλο **Master-Worker**, όπου η διεργασία με `rank 0` (Master) διαχειρίζεται την είσοδο/έξοδο (I/O) και τον αρχικό διαμοιρασμό μνήμης, ενώ όλες οι διεργασίες συμμετέχουν στους υπολογισμούς.

### Διαχείριση Μνήμης & Δεδομένων
* **Data Decomposition:** Οι πίνακες διαχωρίζονται σε οριζόντιες λωρίδες (row-wise decomposition). Κάθε διεργασία είναι υπεύθυνη για $n = N/p$ γραμμές.
* **Contiguous Memory:** Χρησιμοποιείται συνεχόμενη δέσμευση μνήμης για τους δισδιάστατους πίνακες, επιτρέποντας την άμεση αποστολή τους με `MPI_Scatter/Gather` χωρίς την ανάγκη serialization.

### Ανάλυση Αλγορίθμων

#### I. Πρόσθεση Πινάκων ($C + D$)
* **Λογική:** Διαμοιρασμός των πινάκων C και D σε όλους τους επεξεργαστές. Κάθε επεξεργαστής αθροίζει το τοπικό του τμήμα.
* **Επικοινωνία:** `MPI_Scatter` $\rightarrow$ Local Computation $\rightarrow$ `MPI_Gather`.

#### II. Πολλαπλασιασμός Πίνακα-Διανύσματος ($C \times B$)
* **Λογική:** Ο πίνακας C διαμοιράζεται (Scatter). Το διάνυσμα B πρέπει να είναι γνωστό ολόκληρο σε όλους, οπότε εκπέμπεται καθολικά.
* **Επικοινωνία:** `MPI_Bcast` (για το B) και `MPI_Scatter` (για το C).

#### III. Εσωτερικό Γινόμενο ($A \cdot B$)
* **Λογική:** Κάθε επεξεργαστής υπολογίζει το μερικό άθροισμα των στοιχείων που του αναλογούν.
* **Επικοινωνία:** `MPI_Reduce` με τελεστή `MPI_SUM` για την άθροιση των μερικών αποτελεσμάτων στον Master.

#### IV. Πολλαπλασιασμός Πινάκων με Τοπολογία Δακτυλίου ($C \times D$)
Αυτή είναι η πιο σύνθετη λειτουργία. Υλοποιεί έναν **Pipeline Shift αλγόριθμο**:
1.  Κάθε διεργασία διατηρεί σταθερό το κομμάτι του πίνακα **C** που της αναλογεί.
2.  Τα τμήματα του πίνακα **D** κυκλοφορούν ("ρέουν") μεταξύ των διεργασιών σε σχηματισμό δακτυλίου.
3.  Σε κάθε βήμα, η διεργασία εκτελεί πολλαπλασιασμό του τοπικού C με το τρέχον τμήμα του D.
4.  **Non-blocking Communication:** Χρησιμοποιούνται οι εντολές `MPI_Isend` και `MPI_Irecv` για την ταυτόχρονη αποστολή και λήψη των τμημάτων του D, αποφεύγοντας τα deadlocks και μεγιστοποιώντας την ταχύτητα.
5.  **Scalability:** Ο αλγόριθμος έχει γενικευτεί ώστε να λειτουργεί σωστά για οποιοδήποτε $N$ είναι πολλαπλάσιο του $p$ (block cyclic distribution), και όχι μόνο για την περίπτωση $N=p$.

---

## 🔮 Μελλοντικές Επεκτάσεις (Scalability & Optimization)

Η παρούσα υλοποίηση υποθέτει ότι το $N$ διαιρείται ακριβώς με το $p$ ($N \% p = 0$). Παρακάτω αναλύονται τρόποι επέκτασης για καθολική συμβατότητα:

### 1. Υποστήριξη Αυθαίρετων Διαστάσεων ($N \% p \neq 0$)
Για την περίπτωση που οι γραμμές δεν μοιράζονται ακριβώς:
* **Λύση Scatterv/Gatherv:** Αντικατάσταση των απλών `MPI_Scatter/Gather` με τις εκδόσεις `v` (vector). Αυτό επιτρέπει την αποστολή διαφορετικού πλήθους στοιχείων σε κάθε διεργασία (π.χ. οι πρώτες διεργασίες παίρνουν $N/p + 1$ γραμμές και οι υπόλοιπες $N/p$).
* **Padding:** Εναλλακτικά, μπορεί να προστεθούν "εικονικές" γραμμές (με μηδενικά) στον πίνακα ώστε το $N$ να γίνει πολλαπλάσιο του $p$.

### 2. Εύρεση Μέγιστου Στοιχείου (Max Loc)
Για τον εντοπισμό του μέγιστου στοιχείου στον πίνακα αποτελέσματος και της θέσης του:
* Κάθε διεργασία θα υπολογίζει το **Τοπικό Μέγιστο** (τιμή και συντεταγμένες).
* Χρήση της εντολής `MPI_Reduce` με τελεστή **`MPI_MAXLOC`**.
* Η δομή δεδομένων για το reduction θα είναι `{value, rank}`, επιτρέποντας στον Master να γνωρίζει ποια διεργασία έχει το global maximum και σε ποια τοπική θέση.

---

## ⚙️ Οδηγίες Μεταγλώττισης & Εκτέλεσης

### Προαπαιτούμενα
* Μεταγλωττιστής GCC
* Βιβλιοθήκη MPI (π.χ. MPICH ή OpenMPI)

### Compile
```bash
mpicc matrix.c -o matrix
mpiexec -n <number_of_processes> ./matrix
